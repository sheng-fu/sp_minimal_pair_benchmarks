{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b4e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prevot/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f563bb11-af6c-4bc3-9af9-a46899d1fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFYING GENUINE EXAMPLES\n",
    "def move_fp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    filtered_utt = [t for t in utt_as_list if t not in FP_ITEMS]\n",
    "    items_to_move = [t for t in utt_as_list if t in FP_ITEMS]\n",
    "    new_utt = utt_as_list\n",
    "    while new_utt == utt_as_list:\n",
    "        new_utt = filtered_utt\n",
    "        for item in items_to_move:\n",
    "            insert_location = random.randrange(2, len(new_utt)-2)\n",
    "            new_utt.insert(insert_location,item) \n",
    "    return ' '.join(new_utt)\n",
    "\n",
    "\n",
    "def move_sp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    filtered_utt = [t for t in utt_as_list if t not in SP_ITEMS]\n",
    "    items_to_move = [t for t in utt_as_list if t in SP_ITEMS]\n",
    "    new_utt = utt_as_list\n",
    "    while new_utt == utt_as_list:\n",
    "        new_utt = filtered_utt\n",
    "        for item in items_to_move:\n",
    "            insert_location = random.randrange(2, len(new_utt)-2)\n",
    "            new_utt.insert(insert_location,item) \n",
    "    return ' '.join(new_utt)\n",
    "\n",
    "\n",
    "def move_rep(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "\n",
    "    tmp = []\n",
    "    i = 0\n",
    "    while i<len(utt_as_list)-1:\n",
    "        if utt_as_list[i] != utt_as_list[i+1]:\n",
    "            tmp.append(utt_as_list[i])\n",
    "            i+=1\n",
    "        else:\n",
    "            original_repeat_location = i\n",
    "            i+=1\n",
    "    tmp.append(utt_as_list[-1])\n",
    "    location = original_repeat_location\n",
    "    while location == original_repeat_location:\n",
    "        location = random.randint(0, len(tmp))\n",
    "\n",
    "    tmp.insert(location, tmp[location-1])\n",
    "    \n",
    "    return ' '.join(tmp)\n",
    "\n",
    "\n",
    "def replace_fp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    targets = [x for x in utt_as_list if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "    for fp in FP_ITEMS:\n",
    "        utt_as_list = [re.sub(\"^\"+fp+\"$\", random.choice(NOT_FP), x) for x in utt_as_list]\n",
    "    return ' '.join(utt_as_list)\n",
    "\n",
    "def replace_sp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    targets = [x for x in utt_as_list if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "    for fp in SP_ITEMS:\n",
    "        utt_as_list = [re.sub(\"^\"+fp+\"$\", random.choice(NOT_SP), x) for x in utt_as_list]\n",
    "    return ' '.join(utt_as_list)\n",
    "\n",
    "\n",
    "def shuffle_utterance(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    random.shuffle(utt_as_list)\n",
    "    return ' '.join(utt_as_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995031d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BASE\n",
    "N_EXPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a208f",
   "metadata": {},
   "source": [
    "## French\n",
    "### Filled Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed28332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILLED PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_FP = 'data_corpus/french_fp_base_500_checked.txt'\n",
    "FP_ITEMS= ['euh']\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_fp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_fp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cf9fb",
   "metadata": {},
   "source": [
    "### Silent Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c744874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SILENT PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_SP = 'data_corpus/french_sp_base_500_checked.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_sp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_sp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb0049",
   "metadata": {},
   "source": [
    "### REPEATS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42389ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPEATS\n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_RP = 'data_corpus/french_repeats_500.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_rep'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5b1dfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>moved_rep</th>\n",
       "      <th>shuffled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c' était un bar avec euh un un écran ou #</td>\n",
       "      <td>c' était un bar avec avec euh un écran ou #</td>\n",
       "      <td>ou # bar écran un c' avec un un euh était</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en fait elle elle était anglaise</td>\n",
       "      <td>anglaise en fait elle était anglaise</td>\n",
       "      <td>elle elle fait en était anglaise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>style le le la dinde avec euh de la gelée à je...</td>\n",
       "      <td>style le la dinde avec euh euh de la gelée à j...</td>\n",
       "      <td>la avec style sais pas le gelée le à de je quo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ou l' éducation # savait pas trop si c' était ...</td>\n",
       "      <td>ou l' éducation # savait pas trop si c' était ...</td>\n",
       "      <td>marqué a savait pendant éducation était quoi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eux ils restaient très très British quoi</td>\n",
       "      <td>eux eux ils restaient très British quoi</td>\n",
       "      <td>très restaient ils eux British très quoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>donc sur les tu vois les les bénéfi-</td>\n",
       "      <td>donc donc sur les tu vois les bénéfi-</td>\n",
       "      <td>les les sur tu bénéfi- vois les donc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>mais ça me paraissait pas juste quoi de ouais ...</td>\n",
       "      <td>mais ça me paraissait pas juste quoi quoi de o...</td>\n",
       "      <td>eu- j' euh que ça mais fran- sortir paraissait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>du coup quand quand tu arrives je veux dire tu...</td>\n",
       "      <td>du coup quand tu arrives je veux dire tu as au...</td>\n",
       "      <td>du arrives tu * veux # quand dire tu quand je ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>et puis bon mo- moi ça me gêne pas le le truc</td>\n",
       "      <td>et puis bon mo- moi ça me gêne gêne pas le truc</td>\n",
       "      <td>le moi bon mo- le ça pas me et gêne truc puis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>c' est bon la la bourse Fyssen était euh #</td>\n",
       "      <td>c' est bon la bourse bourse Fyssen était euh #</td>\n",
       "      <td>euh bourse bon était la est la # Fyssen c'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0            c' était un bar avec euh un un écran ou #   \n",
       "1                     en fait elle elle était anglaise   \n",
       "2    style le le la dinde avec euh de la gelée à je...   \n",
       "3    ou l' éducation # savait pas trop si c' était ...   \n",
       "4             eux ils restaient très très British quoi   \n",
       "..                                                 ...   \n",
       "495               donc sur les tu vois les les bénéfi-   \n",
       "496  mais ça me paraissait pas juste quoi de ouais ...   \n",
       "497  du coup quand quand tu arrives je veux dire tu...   \n",
       "498      et puis bon mo- moi ça me gêne pas le le truc   \n",
       "499         c' est bon la la bourse Fyssen était euh #   \n",
       "\n",
       "                                             moved_rep  \\\n",
       "0          c' était un bar avec avec euh un écran ou #   \n",
       "1                 anglaise en fait elle était anglaise   \n",
       "2    style le la dinde avec euh euh de la gelée à j...   \n",
       "3    ou l' éducation # savait pas trop si c' était ...   \n",
       "4              eux eux ils restaient très British quoi   \n",
       "..                                                 ...   \n",
       "495              donc donc sur les tu vois les bénéfi-   \n",
       "496  mais ça me paraissait pas juste quoi quoi de o...   \n",
       "497  du coup quand tu arrives je veux dire tu as au...   \n",
       "498    et puis bon mo- moi ça me gêne gêne pas le truc   \n",
       "499     c' est bon la bourse bourse Fyssen était euh #   \n",
       "\n",
       "                                              shuffled  \n",
       "0            ou # bar écran un c' avec un un euh était  \n",
       "1                     elle elle fait en était anglaise  \n",
       "2    la avec style sais pas le gelée le à de je quo...  \n",
       "3    marqué a savait pendant éducation était quoi d...  \n",
       "4             très restaient ils eux British très quoi  \n",
       "..                                                 ...  \n",
       "495               les les sur tu bénéfi- vois les donc  \n",
       "496  eu- j' euh que ça mais fran- sortir paraissait...  \n",
       "497  du arrives tu * veux # quand dire tu quand je ...  \n",
       "498      le moi bon mo- le ça pas me et gêne truc puis  \n",
       "499         euh bourse bon était la est la # Fyssen c'  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REP_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb7673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7663495",
   "metadata": {},
   "source": [
    "# DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15f11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dm(row):\n",
    "    right_as_list = row['right'].split()[1:]\n",
    "    dm_candidates = [dm for dm in DMS if dm != row['dm']]\n",
    "    random.shuffle(dm_candidates)\n",
    "    return row['left'] + ' ' + dm_candidates[0] + ' ' + ' '.join(right_as_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data_benchmark/discourse/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_DM_MONO = 'data_corpus/french_DM_mono.txt'\n",
    "FILE_DM_DIAL = 'data_corpus/french_DM_dial.txt'\n",
    "DMS = ['mais', 'donc', 'parce_que', 'enfin', 'alors','puis','bon','après','puisque','ben']\n",
    "\n",
    "\n",
    "DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_MONO,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','dm','right'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    DM_DF['text'] = DM_DF['left'] + ' '+  DM_DF['right']\n",
    "    DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,axis=1)\n",
    "    DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in DM_DF.iterrows():\n",
    "        item_replaced = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_dm'], \"field\": \"discourse\", \n",
    "                         \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_replaced)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'replaced_dm_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in DM_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['shuffled'], \"field\": \"discourse\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "                    \n",
    "    with open(folder + 'shuffled_dm_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd404a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ec052-287f-458b-b94b-52340050eeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f25863-bb70-4c35-bff4-3cc74485cfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc10e4-3c41-4df9-833e-7bea5cc96242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
