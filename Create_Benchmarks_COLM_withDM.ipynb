{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 356,
>>>>>>> 0ac602f5636a1bf46559d068f59015fcabf3499f
   "id": "61b4e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 357,
>>>>>>> 0ac602f5636a1bf46559d068f59015fcabf3499f
   "id": "f563bb11-af6c-4bc3-9af9-a46899d1fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFYING GENUINE EXAMPLES\n",
    "def move_fp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    filtered_utt = [t for t in utt_as_list if t not in FP_ITEMS]\n",
    "    items_to_move = [t for t in utt_as_list if t in FP_ITEMS]\n",
    "    new_utt = utt_as_list\n",
    "    while new_utt == utt_as_list:\n",
    "        new_utt = filtered_utt\n",
    "        for item in items_to_move:\n",
    "            insert_location = random.randrange(2, len(new_utt)-2)\n",
    "            new_utt.insert(insert_location,item) \n",
    "    return ' '.join(new_utt)\n",
    "\n",
    "\n",
    "def move_sp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    filtered_utt = [t for t in utt_as_list if t not in SP_ITEMS]\n",
    "    items_to_move = [t for t in utt_as_list if t in SP_ITEMS]\n",
    "    new_utt = utt_as_list\n",
    "    while new_utt == utt_as_list:\n",
    "        new_utt = filtered_utt\n",
    "        for item in items_to_move:\n",
    "            insert_location = random.randrange(2, len(new_utt)-2)\n",
    "            new_utt.insert(insert_location,item) \n",
    "    return ' '.join(new_utt)\n",
    "\n",
    "\n",
    "def move_rep(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    original_repeat_location = 0\n",
    "    tmp = []\n",
    "    i = 0\n",
    "    while i<len(utt_as_list)-1:\n",
    "        if utt_as_list[i] != utt_as_list[i+1]:\n",
    "            tmp.append(utt_as_list[i])\n",
    "            i+=1\n",
    "        else:\n",
    "            original_repeat_location = i\n",
    "            i+=1\n",
    "    tmp.append(utt_as_list[-1])\n",
    "    location = original_repeat_location\n",
    "    while location == original_repeat_location:\n",
    "        location = random.randint(0, len(tmp))\n",
    "\n",
    "    tmp.insert(location, tmp[location-1])\n",
    "    \n",
    "    return ' '.join(tmp)\n",
    "\n",
    "\n",
    "def replace_fp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    targets = [x for x in utt_as_list if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "    for fp in FP_ITEMS:\n",
    "        utt_as_list = [re.sub(\"^\"+fp+\"$\", random.choice(NOT_FP), x) for x in utt_as_list]\n",
    "    return ' '.join(utt_as_list)\n",
    "\n",
    "def replace_sp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    targets = [x for x in utt_as_list if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "    for fp in SP_ITEMS:\n",
    "        utt_as_list = [re.sub(\"^\"+fp+\"$\", random.choice(NOT_SP), x) for x in utt_as_list]\n",
    "    return ' '.join(utt_as_list)\n",
    "\n",
    "\n",
    "def shuffle_utterance(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "    random.shuffle(utt_as_list)\n",
    "    return ' '.join(utt_as_list)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 358,
>>>>>>> 0ac602f5636a1bf46559d068f59015fcabf3499f
   "id": "995031d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BASE\n",
    "N_EXPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a208f",
   "metadata": {},
   "source": [
    "## French\n",
    "### Filled Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ed28332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILLED PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_FP = 'data_corpus/french_fp_base_500_checked.txt'\n",
    "FP_ITEMS= ['euh']\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_fp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_fp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['shuffled'])\n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['moved_fp'])\n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['replaced_fp'])\n",
    "        item_removed = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cf9fb",
   "metadata": {},
   "source": [
    "### Silent Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c744874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SILENT PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_SP = 'data_corpus/french_sp_base_500_checked.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_sp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_sp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['shuffled'])\n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['moved_sp'])\n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['replaced_sp'])\n",
    "        item_removed = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb0049",
   "metadata": {},
   "source": [
    "### REPEATS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "42389ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPEATS\n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_RP = 'data_corpus/french_repeats_500.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_rep'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "\n",
    "\n",
    "### REPEATS\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['shuffled'])        \n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['moved_rep'])       \n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e5b1dfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>moved_rep</th>\n",
       "      <th>shuffled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c' était un bar avec euh un un écran ou #</td>\n",
       "      <td>c' était un bar bar avec euh un écran ou #</td>\n",
       "      <td>avec était # c' bar ou un un écran un euh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en fait elle elle était anglaise</td>\n",
       "      <td>anglaise en fait elle était anglaise</td>\n",
       "      <td>elle anglaise en elle fait était</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>style le le la dinde avec euh de la gelée à je...</td>\n",
       "      <td>style le la dinde avec euh de la gelée à je sa...</td>\n",
       "      <td>quoi le gelée de le la je à trop style euh la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ou l' éducation # savait pas trop si c' était ...</td>\n",
       "      <td>ou l' éducation # savait pas trop si c' était ...</td>\n",
       "      <td>euh ça pas ou temps ou éducation était quoi tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eux ils restaient très très British quoi</td>\n",
       "      <td>eux ils ils restaient très British quoi</td>\n",
       "      <td>très très eux British quoi ils restaient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>donc sur les tu vois les les bénéfi-</td>\n",
       "      <td>donc donc sur les tu vois les bénéfi-</td>\n",
       "      <td>sur les les vois les bénéfi- tu donc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>mais ça me paraissait pas juste quoi de ouais ...</td>\n",
       "      <td>mais ça me paraissait pas juste quoi de ouais ...</td>\n",
       "      <td>alors @ de juste pas de me de paraissait ça eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>du coup quand quand tu arrives je veux dire tu...</td>\n",
       "      <td>du coup quand tu arrives je veux veux dire tu ...</td>\n",
       "      <td>coup # tu aussi je quand euh arrives du veux *...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>et puis bon mo- moi ça me gêne pas le le truc</td>\n",
       "      <td>et puis bon mo- moi ça me me gêne pas le truc</td>\n",
       "      <td>pas le bon mo- le et me ça truc puis gêne moi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>c' est bon la la bourse Fyssen était euh #</td>\n",
       "      <td>c' est est bon la bourse Fyssen était euh #</td>\n",
       "      <td>la euh c' bourse est la était # Fyssen bon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0            c' était un bar avec euh un un écran ou #   \n",
       "1                     en fait elle elle était anglaise   \n",
       "2    style le le la dinde avec euh de la gelée à je...   \n",
       "3    ou l' éducation # savait pas trop si c' était ...   \n",
       "4             eux ils restaient très très British quoi   \n",
       "..                                                 ...   \n",
       "495               donc sur les tu vois les les bénéfi-   \n",
       "496  mais ça me paraissait pas juste quoi de ouais ...   \n",
       "497  du coup quand quand tu arrives je veux dire tu...   \n",
       "498      et puis bon mo- moi ça me gêne pas le le truc   \n",
       "499         c' est bon la la bourse Fyssen était euh #   \n",
       "\n",
       "                                             moved_rep  \\\n",
       "0           c' était un bar bar avec euh un écran ou #   \n",
       "1                 anglaise en fait elle était anglaise   \n",
       "2    style le la dinde avec euh de la gelée à je sa...   \n",
       "3    ou l' éducation # savait pas trop si c' était ...   \n",
       "4              eux ils ils restaient très British quoi   \n",
       "..                                                 ...   \n",
       "495              donc donc sur les tu vois les bénéfi-   \n",
       "496  mais ça me paraissait pas juste quoi de ouais ...   \n",
       "497  du coup quand tu arrives je veux veux dire tu ...   \n",
       "498      et puis bon mo- moi ça me me gêne pas le truc   \n",
       "499        c' est est bon la bourse Fyssen était euh #   \n",
       "\n",
       "                                              shuffled  \n",
       "0            avec était # c' bar ou un un écran un euh  \n",
       "1                     elle anglaise en elle fait était  \n",
       "2    quoi le gelée de le la je à trop style euh la ...  \n",
       "3    euh ça pas ou temps ou éducation était quoi tr...  \n",
       "4             très très eux British quoi ils restaient  \n",
       "..                                                 ...  \n",
       "495               sur les les vois les bénéfi- tu donc  \n",
       "496  alors @ de juste pas de me de paraissait ça eu...  \n",
       "497  coup # tu aussi je quand euh arrives du veux *...  \n",
       "498      pas le bon mo- le et me ça truc puis gêne moi  \n",
       "499         la euh c' bourse est la était # Fyssen bon  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REP_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb1430-35e6-4a14-b55f-042ad7661ab1",
   "metadata": {},
   "source": [
    "## English\n",
    "### Filled Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "9867acb9-a2f0-4bf6-9515-5257520ef4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILLED PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# EN\n",
    "LAN = 'en'\n",
    "FILE_FP = 'data_corpus/english_fp_base_500_checked.txt'\n",
    "FP_ITEMS= ['uh', 'um', 'Uh', 'Um']\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_fp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_fp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['shuffled'])\n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['moved_fp'])\n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['replaced_fp'])\n",
    "        item_removed = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3585f-a720-432d-b3d4-68035e2d5cd1",
   "metadata": {},
   "source": [
    "### Silent Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "10fcb11a-c792-4788-aae2-32dd8aade733",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SILENT PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# EN\n",
    "LAN = 'en'\n",
    "FILE_SP = 'data_corpus/english_sp_base_500_checked.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_sp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_sp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['shuffled'])\n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['moved_sp'])\n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['replaced_sp'])\n",
    "        item_removed = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "139fcb43-75ba-4a7e-a5fd-93748858e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1def8-ade6-423c-8c3a-bfc3156aabde",
   "metadata": {},
   "source": [
    "### REPEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "73331144-ca77-4b62-bb5a-a34b00807172",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPEATS\n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# EN\n",
    "LAN = 'en'\n",
    "FILE_RP = 'data_corpus/english_repeats_base_500_checked.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_rep'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "\n",
    "\n",
    "### REPEATS\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['shuffled'])        \n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        good = re.sub(' *#', ',', row['text'])\n",
    "        bad = re.sub(' *#', ',', row['moved_rep'])       \n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ee7a8-8d27-48ad-948b-c7c156986d1c",
   "metadata": {},
   "source": [
    "## Mandarin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e7deeaed-8a39-464b-8dbd-8dfc39655c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    " 'E':'欸',\n",
    " 'EI':'欸',\n",
    " 'NE':'那',\n",
    " 'NEGE':'那個',  'NE GE':'那個',\n",
    " 'NEI':'那',\n",
    " 'NEIGE':'那個',  'NEI GE':'那個',\n",
    " 'NEIN':'那',\n",
    " 'ZHE':'這',\n",
    " 'ZHEGE':'這個',\n",
    " 'ZHEIGE':'這個',  'ZHE GE':'這個',  'ZHE':'這',  \n",
    "     'ZHEI GE':'這個', 'ZHEI': '這',\n",
    "      'ZHEI':'這'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0caae4-10e4-4f74-b20a-4d81a6bedf7d",
   "metadata": {},
   "source": [
    "\n",
    "### Filled Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "a507f68c-3382-4443-829c-5581e162dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILLED PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# ZH\n",
    "LAN = 'zh'\n",
    "FILE_FP = 'data_corpus/mandarin_fp_base_checked.txt'\n",
    "FP_ITEMS= ['uhm', 'mhm', 'uhn', 'en', 'E', 'EI', 'ein', 'NEGE', 'NEIGE', 'nhn']\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_fp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_fp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "\n",
    "FP_EXAMPLES = [u[:-1] for u in open(FILE_FP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_FP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_FP = [x for x in f.read().split() if x not in FP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "FP_DF = pd.DataFrame(FP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    FP_DF['moved_fp'] = FP_DF.apply(move_fp,axis=1)\n",
    "    #FP_DF['removed_fp'] = FP_DF.apply(remove_fp,axis=1)\n",
    "    FP_DF['replaced_fp'] = FP_DF.apply(replace_fp,axis=1)\n",
    "    FP_DF['shuffled'] = FP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['shuffled'])\n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['moved_fp'])\n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_filler_\"+LAN, \"UID\": \"moved_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in FP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['replaced_fp'])\n",
    "        item_removed = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_filler_\"+LAN, \"UID\": \"replaced_filler_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_filler_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b652b21-1a8d-4756-a9b3-550c4ea27329",
   "metadata": {},
   "source": [
    "### Silent Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ad36ad6d-479a-488f-8e12-b219d1738167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SILENT PAUSES \n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "# FR\n",
    "LAN = 'zh'\n",
    "FILE_SP = 'data_corpus/mandarin_sp_base_500_checked.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_sp'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        item_removed = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_sp'], \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "\n",
    "SP_EXAMPLES = [u[:-1] for u in open(FILE_SP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "with open(FILE_SP, 'r', encoding = 'utf-8') as f:\n",
    "    NOT_SP = [x for x in f.read().split() if x not in SP_ITEMS + ['#', '*', ',']]\n",
    "\n",
    "SP_DF = pd.DataFrame(SP_EXAMPLES,columns=['text'])\n",
    "\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    SP_DF['moved_sp'] = SP_DF.apply(move_sp,axis=1)\n",
    "    #SP_DF['removed_sp'] = SP_DF.apply(remove_sp,axis=1)\n",
    "    SP_DF['replaced_sp'] = SP_DF.apply(replace_sp,axis=1)\n",
    "    SP_DF['shuffled'] = SP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['shuffled'])\n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['moved_sp'])\n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_pause_\"+LAN, \"UID\": \"moved_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in SP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['replaced_sp'])\n",
    "        item_removed = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                        \"linguistics_term\": \"replaced_pause_\"+LAN, \"UID\": \"replaced_pause_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                        \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                        \"lexically_identical\": False, \"pair_id\": str(cnt)}\n",
    "        result.append(item_removed)\n",
    "        cnt +=1\n",
    "        \n",
    "    with open(folder + 'replaced_pause_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "35a9c0d6-91a9-441e-b7c3-25b5daba7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8ccbe-d352-4cbd-81b4-54678ede20b8",
   "metadata": {},
   "source": [
    "### REPEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "901b3228-6084-454b-9ae9-0d14db7e0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPEATS\n",
    "\n",
    "folder = './data_benchmark/disfl/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# ZH\n",
    "LAN = 'zh'\n",
    "FILE_RP = 'data_corpus/mandarin_repeats_base_500_checked.txt'\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_shuffled = {\"sentence_good\":row['text'],\"sentence_bad\":row['shuffled'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['moved_rep'], \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "\n",
    "folder = './data_benchmark/disfl_comma/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "REP_EXAMPLES = [u[:-1] for u in open(FILE_RP,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "REP_DF = pd.DataFrame(REP_EXAMPLES,columns=['text'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    REP_DF['moved_rep'] = REP_DF.apply(move_rep,axis=1)\n",
    "    REP_DF['shuffled'] = REP_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['shuffled'])        \n",
    "        item_shuffled = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": True, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_shuffled)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'shuffled_repeat_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # MOVED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in REP_DF.iterrows():\n",
    "        good = re.sub(' *#', '，', row['text'])\n",
    "        bad = re.sub(' *#', '，', row['moved_rep'])       \n",
    "        item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"disfluencies\", \n",
    "                         \"linguistics_term\": \"moved_rep_\"+LAN, \"UID\": \"moved_rep_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'moved_rep_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "1085cd83-f4c1-4c44-957f-a6c2742462a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505870ad-9ddd-41c8-a58e-d3050e3f062b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "c7663495",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03e63e-e0cc-4f05-ae15-2a57d184d5bd",
>>>>>>> 0ac602f5636a1bf46559d068f59015fcabf3499f
   "metadata": {},
   "source": [
    "# DM"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "c15f11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dm(row):\n",
    "    right_as_list = row['right'].split()[1:]\n",
    "    dm_candidates = [dm for dm in DMS if dm != row['dm']]\n",
    "    random.shuffle(dm_candidates)\n",
    "    return row['left'] + ' ' + dm_candidates[0] + ' ' + ' '.join(right_as_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data_benchmark/discourse/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_DM_MONO = 'data_corpus/french_DM_mono.txt'\n",
    "FILE_DM_DIAL = 'data_corpus/french_DM_dial.txt'\n",
    "DMS = ['mais', 'donc', 'parce_que', 'enfin', 'alors','puis','bon','après','puisque','ben']\n",
    "\n",
    "\n",
    "DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_MONO,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "\n",
    "DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','dm','right'])\n",
    "\n",
    "for i in range(N_EXPS):\n",
    "    DM_DF['text'] = DM_DF['left'] + ' '+  DM_DF['right']\n",
    "    DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,axis=1)\n",
    "    DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "    \n",
    "    exp_tag = LAN+'_'+str(i)\n",
    "    \n",
    "    #REPLACED\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in DM_DF.iterrows():\n",
    "        item_replaced = {\"sentence_good\":row['text'],\"sentence_bad\":row['replaced_dm'], \"field\": \"discourse\", \n",
    "                         \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                         \"pair_id\": str(cnt)}\n",
    "        result.append(item_replaced)\n",
    "        cnt +=1\n",
    "            \n",
    "    with open(folder + 'replaced_dm_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "    # SHUFFLED    \n",
    "    result = []\n",
    "    cnt=0\n",
    "    for i,row in DM_DF.iterrows():\n",
    "        item_moved = {\"sentence_good\":row['text'], \"sentence_bad\":row['shuffled'], \"field\": \"discourse\", \n",
    "                         \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                         \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                         \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "        result.append(item_moved)\n",
    "        cnt +=1\n",
    "                    \n",
    "    with open(folder + 'shuffled_dm_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "        for entry in result:\n",
    "            json.dump(entry, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd404a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ec052-287f-458b-b94b-52340050eeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f25863-bb70-4c35-bff4-3cc74485cfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc10e4-3c41-4df9-833e-7bea5cc96242",
=======
   "execution_count": null,
   "id": "4fd4e9a0-58c4-46db-90d7-1f4b2dfd5c15",
>>>>>>> 0ac602f5636a1bf46559d068f59015fcabf3499f
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
