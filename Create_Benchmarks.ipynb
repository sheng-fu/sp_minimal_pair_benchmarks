{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "61b4e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(20250409)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f563bb11-af6c-4bc3-9af9-a46899d1fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFYING GENUINE EXAMPLES\n",
    "def move_fp(row):\n",
    "\n",
    "    utt_as_list = row['text'].split()\n",
    "            \n",
    "    filtered_utt_ = [t for t in utt_as_list if t not in FP_ITEMS]\n",
    "    items_to_move = [t for t in utt_as_list if t in FP_ITEMS]\n",
    "\n",
    "    ids_to_move = [x for x in range(len(utt_as_list)) if utt_as_list[x] in FP_ITEMS]\n",
    "\n",
    "    left_context = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x-1 for x in ids_to_move] ]\n",
    "    right_context = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x+1 for x in ids_to_move] ]\n",
    "    targets = list(zip(left_context, items_to_move, right_context))\n",
    "\n",
    "    new_utt = utt_as_list\n",
    "    while new_utt == utt_as_list or '# #' in ' '.join(new_utt):\n",
    "        new_utt = [t for t in utt_as_list if t not in FP_ITEMS]\n",
    "        moved_targets = []\n",
    "        for item in items_to_move:\n",
    "            insert_location = random.randrange(1, len(new_utt)-1)\n",
    "            new_utt.insert(insert_location,item)  \n",
    "            moved_targets.append((new_utt[insert_location-1], item, new_utt[insert_location+1]))\n",
    "\n",
    "    return ' '.join(new_utt), targets, moved_targets\n",
    "\n",
    "def move_sp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "\n",
    "    filtered_utt = [t for t in utt_as_list if t not in SP_ITEMS]\n",
    "    items_to_move = [t for t in utt_as_list if t in SP_ITEMS]\n",
    "\n",
    "    ids_to_move = [x for x in range(len(utt_as_list)) if utt_as_list[x] in SP_ITEMS]\n",
    "\n",
    "    left_context = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x-1 for x in ids_to_move] ]\n",
    "    right_context = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x+1 for x in ids_to_move] ]\n",
    "    targets = list(zip(left_context, items_to_move, right_context))\n",
    "\n",
    "    \n",
    "    new_utt = utt_as_list\n",
    "    while new_utt == utt_as_list or '# #' in ' '.join(new_utt):\n",
    "        new_utt = [t for t in utt_as_list if t not in SP_ITEMS]\n",
    "        moved_targets = []\n",
    "        for item in items_to_move:\n",
    "            insert_location = random.randrange(1,len(new_utt)-1)\n",
    "            new_utt.insert(insert_location,item) \n",
    "            moved_targets.append((new_utt[insert_location-1], item, new_utt[insert_location+1]))\n",
    "\n",
    "\n",
    "    return ' '.join(new_utt), targets, moved_targets\n",
    "\n",
    "\n",
    "\n",
    "def move_rep(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "\n",
    "\n",
    "    repeated_indices = []\n",
    "    i = 1\n",
    "    while i<len(utt_as_list):\n",
    "        if (utt_as_list[i] == utt_as_list[i-1]) and (utt_as_list[i] not in SP_ITEMS):\n",
    "            repeated_indices.append(i)\n",
    "        i+=1\n",
    "    #tmp.append(utt_as_list[-1])\n",
    "\n",
    "    rep_to_move = random.choice(repeated_indices)\n",
    "    rep_to_move_token = utt_as_list[rep_to_move]\n",
    "    left_context = utt_as_list[rep_to_move-2] if rep_to_move > 1 else ''\n",
    "    right_context = utt_as_list[rep_to_move+1] if rep_to_move < len(utt_as_list)-1 else ''\n",
    "    \n",
    "    utt_as_list[rep_to_move] = ''\n",
    "    \n",
    "    avoid =  [rep_to_move + 1, rep_to_move] + [x + 1 for x in range(len(utt_as_list)-1) if utt_as_list[x] in SP_ITEMS + FP_ITEMS + ['*']]\n",
    "    \n",
    "    original_length = len(utt_as_list)\n",
    "    newlist_length = 0\n",
    "\n",
    "    while original_length != newlist_length:\n",
    "    \n",
    "        location = random.choice([x for x in list(range(1,len(utt_as_list)+1)) if x not in avoid])\n",
    "        #print(utt_as_list, location)\n",
    "        rep_moved = utt_as_list[location-1]\n",
    "        left_moved = utt_as_list[location-2] if location > 1 else ''\n",
    "        right_moved = utt_as_list[location] if location < len(utt_as_list) else ''\n",
    "        \n",
    "        utt_as_list.insert(location, utt_as_list[location-1])\n",
    "        utt_as_list = [x for x in utt_as_list if x != '']\n",
    "\n",
    "        newlist_length = len(utt_as_list)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return ' '.join(utt_as_list), [(left_context, rep_to_move_token, right_context)], [(left_moved, rep_moved, right_moved)]\n",
    "\n",
    "\n",
    "\n",
    "def swap_sp_fp(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "\n",
    "    fp_positions = [x for x in range(len(utt_as_list)) if utt_as_list[x] in FP_ITEMS]\n",
    "    sp_positions = [x for x in range(len(utt_as_list)) if utt_as_list[x] in SP_ITEMS]\n",
    "\n",
    "    fp_pos = random.choice(fp_positions)\n",
    "    sp_pos = random.choice(sp_positions)\n",
    "    fp_to_switch = utt_as_list[fp_pos] \n",
    "    sp_to_switch = utt_as_list[sp_pos] \n",
    "\n",
    "    utt_as_list[fp_pos] = sp_to_switch\n",
    "    utt_as_list[sp_pos] = fp_to_switch\n",
    "\n",
    "    left_context = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x-1 for x in [fp_pos]]]\n",
    "    right_context = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x+1 for x in [fp_pos]]]\n",
    "    targets = list(zip(left_context, [fp_to_switch], right_context))\n",
    "\n",
    "    left_moved = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x-1 for x in [sp_pos]]]\n",
    "    right_moved = [utt_as_list[x] if x >= 0 and x < len(utt_as_list) else '' for x in [x+1 for x in [sp_pos]]]\n",
    "    targets_moved = list(zip(left_moved, [sp_to_switch], right_moved)) \n",
    "    \n",
    "    return ' '.join(utt_as_list), targets, targets_moved\n",
    "\n",
    "\n",
    "def shuffle_utterance(row):\n",
    "    utt_as_list = row['text'].split()\n",
    "\n",
    "    random.shuffle(utt_as_list)\n",
    "\n",
    "\n",
    "    return ' '.join(utt_as_list), [], []\n",
    "\n",
    "# EN\n",
    "def lowercase_except_I_and_contractions(text):\n",
    "    return re.sub(r\"\\b(?!I\\b|I'm\\b|I've\\b|I'll\\b|I'd\\b)[A-HIJ-Z][a-z]*\\b\", lambda match: match.group(0).lower(), text)\n",
    "\n",
    "\n",
    "def replace_dm(row,dmlist,sep = ' # '):\n",
    "    right_as_list = row['right'].split()[1:]\n",
    "\n",
    "    if isinstance(dmlist, list):\n",
    "        dm_candidates = [dm for dm in dmlist if dm != row['dm'].strip()]\n",
    "    else:\n",
    "        dm_candidates = [dm for dm in dmlist[row['dm'].strip()]]\n",
    "    \n",
    "    random.shuffle(dm_candidates)\n",
    "\n",
    "    left_as_list = row['left'].split()\n",
    "    if left_as_list[0] == '#':\n",
    "        left_as_list = left_as_list[1:]\n",
    "    \n",
    "    output = ' '.join(left_as_list).strip() + sep + dm_candidates[0] + ' ' + ' '.join(right_as_list).strip()\n",
    "    return output, [(left_as_list[-1], row['right'].split()[0], right_as_list[0])], [(left_as_list[-1], dm_candidates[0], right_as_list[0])]\n",
    "\n",
    "\n",
    "def get_dm(row):\n",
    "    return row['right'].split()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b118036e-bc58-42e9-b8c4-e0804f82361e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = {}\n",
    "isinstance(a, list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c6c5b34c-4e9f-4d06-9eff-0f518667a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "'NEGE':'那個', 'NEINGE':'那個',\n",
    " 'NEGE':'那個',  'NE GE':'那個',\n",
    " 'ZHEGE':'這個',     'ZHEI GE':'這個',\n",
    " 'ZHEIGE':'這個',  'ZHE GE':'這個', \n",
    " 'NEIGE':'那個',  'NEI GE':'那個',  'NAGE':'那個',\n",
    " 'NEIN':'那', 'NA': '那',\n",
    " 'ZHE':'這', 'GE':'個',\n",
    " 'ZHEI': '這',\n",
    " 'ZHEI':'這',  'ein':'eh',  'en':'嗯',\n",
    " 'E':'eh',\n",
    " 'EI':'eh',\n",
    " 'NE':'那',  'NEI':'那', 'ZHE':'這',  \n",
    " 'mhm':'mhm','nhn': '嗯', 'uhm':'嗯', 'uhn':'嗯', \n",
    "}\n",
    "\n",
    "\n",
    "# Define a more comprehensive Chinese character range covering all CJK blocks\n",
    "CHINESE_RANGE = r'\\u4e00-\\u9fff\\uf900-\\ufaff\\u3400-\\u4dbf\\U00020000-\\U0002A6DF\\U0002A700-\\U0002B73F\\U0002B740-\\U0002B81F\\U0002B820-\\U0002CEAF'\n",
    "\n",
    "# Define Chinese punctuation range (including 「」【】 and others)\n",
    "PUNCTUATION_RANGE = r'().,!?;，。！？；：:︰、\\[\\]（）〔〕【】「」『』《》”“－…<＞/%％*@\"'\n",
    "CHINESE_PUNCTUATION_RANGE = r'()，。！？；：︰、\\[\\]（）〔〕【】「」『』《》”“－…<＞0123456789\"A-Za-z*@'\n",
    "\n",
    "def clean_zh_space(text):\n",
    "    \n",
    "    for key in mapping.keys():\n",
    "        text = re.sub(\"(^| )\" + key + \"($| )\", \"\\\\1\"+mapping[key]+\"\\\\2\", text)\n",
    "        text = re.sub(\"(^| )\" + key + \"($| )\", \"\\\\1\"+mapping[key]+\"\\\\2\", text)\n",
    "        \n",
    "    text = re.sub(rf'(?<=[{CHINESE_RANGE+'*'}]) (?=[{CHINESE_RANGE+'*'}])', '', text)\n",
    "    text = re.sub(rf'(?<=[{CHINESE_PUNCTUATION_RANGE}]) (?=[{CHINESE_RANGE}])', '', text)\n",
    "    text = re.sub(rf'(?<=[{CHINESE_RANGE}]) (?=[{CHINESE_PUNCTUATION_RANGE}])', '', text)\n",
    "    text = re.sub(rf'(?<=[{CHINESE_RANGE}]) (?=[{PUNCTUATION_RANGE}])', '', text)\n",
    "\n",
    "    text = re.sub(' *# *', '#', text)\n",
    "    text = re.sub(' *\\\\* *', '*', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6d23125d-172a-469d-ae9c-1aa0becb1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {\n",
    "    'filler_moved': move_fp,\n",
    "    'filler_swapped': swap_sp_fp,\n",
    "    'pause_moved': move_sp,\n",
    "    'rep_moved': move_rep,\n",
    "    'filler_shuffled': shuffle_utterance,\n",
    "    'pause_shuffled': shuffle_utterance,\n",
    "    'rep_shuffled': shuffle_utterance,   \n",
    "    'dm_dial_sem_shuffled': shuffle_utterance,\n",
    "    'dm_dial_att_shuffled': shuffle_utterance,\n",
    "    'dm_mono_sem_shuffled': shuffle_utterance,\n",
    "    'dm_mono_att_shuffled': shuffle_utterance,  \n",
    "    'dm_dial_sem_replaced': replace_dm,\n",
    "    'dm_dial_att_replaced': replace_dm,\n",
    "    'dm_mono_sem_replaced': replace_dm,\n",
    "    'dm_mono_att_replaced': replace_dm,  \n",
    "}\n",
    "\n",
    "def filter_for_swap(EXAMPLES, fp_list, sp_list):\n",
    "\n",
    "    good_example_no = []\n",
    "\n",
    "    for x in range(len(EXAMPLES)):\n",
    "        if EXAMPLES[x].split(' ')[-1] == '#':\n",
    "            EXAMPLES[x] = ' '.join(EXAMPLES[x].split(' ')[:-1])\n",
    "\n",
    "    for f in range(len(EXAMPLES)):\n",
    "        FP_locs = [x for x in range(len(EXAMPLES[f].split())) if EXAMPLES[f].split()[x] in FP_ITEMS]\n",
    "        SP_locs = [x for x in range(len(EXAMPLES[f].split())) if EXAMPLES[f].split()[x] in SP_ITEMS]\n",
    "        if len(SP_locs) >= 1 and len(FP_locs) >= 1:\n",
    "            differences = [abs(a - b) for a in FP_locs for b in SP_locs]\n",
    "            if 1 not in differences:\n",
    "                good_example_no.append(f)\n",
    "      \n",
    "    \n",
    "    EXAMPLES = [EXAMPLES[x] for x in range(len(EXAMPLES)) if x in good_example_no and '#' in EXAMPLES[x]]\n",
    "\n",
    "    return EXAMPLES\n",
    "\n",
    "\n",
    "\n",
    "def run_gen(task, lan, base_file, fp_list, sp_list, out_folder, comma_pattern, n_exps, dm_list=[]):\n",
    "\n",
    "    #create the output benchmark path if not existing\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "\n",
    "\n",
    "    for i_exp in range(n_exps):\n",
    "        if 'dm' in task:\n",
    "            EXAMPLES = [u.split('<$>') for u in open(base_file,'r', encoding = 'utf-8').readlines()]\n",
    "            if lan == 'fr':\n",
    "                DF = pd.DataFrame(EXAMPLES,columns=['left','dm','right'])\n",
    "                DF['left'] = DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "                DF['right'] = DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)        \n",
    "                DF['text'] = DF['left'].str.strip() + ' # '+  DF['right'].str.strip()\n",
    "            else:\n",
    "                DF = pd.DataFrame(EXAMPLES,columns=['left','right'])\n",
    "                DF['dm'] = DF.apply(get_dm,axis=1)       \n",
    "                DF['text'] = DF['left'].str.strip() + ' # '+  DF['right'].str.strip()                \n",
    "        else:\n",
    "            EXAMPLES = [u[:-1] for u in open(base_file,'r', encoding = 'utf-8').readlines()]\n",
    "            EXAMPLES = [x for x in EXAMPLES if '# #' not in x]\n",
    "            if 'swap' in task:\n",
    "                EXAMPLES = filter_for_swap(EXAMPLES, fp_list, sp_list)    \n",
    "            DF = pd.DataFrame(EXAMPLES,columns=['text'])\n",
    "\n",
    "        if lan == 'en':\n",
    "            DF['text'] = DF['text'].apply(lowercase_except_I_and_contractions)\n",
    "\n",
    "        if 'dm' in task and 'replaced' in task:\n",
    "            DF[task] = DF.apply(task_dict[task],dmlist=dm_list,axis=1)\n",
    "        else:            \n",
    "            DF[task] = DF.apply(task_dict[task],axis=1)\n",
    "        \n",
    "        exp_tag = lan+'_'+str(i_exp)\n",
    "\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DF.iterrows():\n",
    "            good = row['text']\n",
    "            bad = row[task][0]\n",
    "\n",
    "            if lan == 'zh':\n",
    "                good = clean_zh_space(good)\n",
    "                bad = clean_zh_space(bad)\n",
    "            elif lan == 'fr':\n",
    "                good = re.sub(r\"\\b(c|d|j|l|m|n|s|t|qu)'\\s+(?=[aeiouyhAEIOUYHàâäéèêëîïôöùûüÿœæÀÂÄÉÈÊËÎÏÔÖÙÛÜŸŒÆ])\", r\"\\1'\", good)\n",
    "                bad = re.sub(r\"\\b(c|d|j|l|m|n|s|t|qu)'\\s+(?=[aeiouyhAEIOUYHàâäéèêëîïôöùûüÿœæÀÂÄÉÈÊËÎÏÔÖÙÛÜŸŒÆ])\", r\"\\1'\", bad)\n",
    "                \n",
    "            good = re.sub('( *)#', comma_pattern, good)\n",
    "            bad = re.sub('( *)#', comma_pattern, bad)\n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": task, \n",
    "                             \"linguistics_term\": task+\"_\"+LAN, \"UID\": task+\"_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt), 'targets': row[task][1], 'moved_targets': row[task][2]}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(out_folder + task + '_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d5099e22-1c98-46ed-a508-c18d4596e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAN = 'fr'\n",
    "FP_ITEMS= ['euh']\n",
    "SP_ITEMS = ['#']\n",
    "FILE_FP = 'data_corpus/french_fp_base_500_checked.txt'\n",
    "comma = ','\n",
    "\n",
    "run_gen('filler_moved', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('filler_shuffled', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('filler_swapped', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "\n",
    "FILE_REP = 'data_corpus/french_repeats_500.txt'\n",
    "\n",
    "run_gen('rep_moved', LAN, FILE_REP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('rep_shuffled', LAN, FILE_REP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "\n",
    "FILE_SP = 'data_corpus/french_sp_base_500_checked.txt'\n",
    "\n",
    "run_gen('pause_moved', LAN, FILE_SP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/',comma, 10)\n",
    "run_gen('pause_shuffled', LAN, FILE_SP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/',comma, 10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4070e4a1-1788-4e1e-9b64-9514e122c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_SEM = ['donc','mais','alors','après'] \n",
    "DM_ATT =  [\"ah\", \"ben\", \"oh\", \"enfin\", \"bon\"]#[\"ah\", \"ben\"]\n",
    "\n",
    "FILE_DM_DIAL_SEM = 'data_corpus/french_DM_sem_dial.txt'\n",
    "FILE_DM_MONO_SEM = 'data_corpus/french_DM_sem_mono.txt'\n",
    "FILE_DM_DIAL_ATT = 'data_corpus/french_DM_att_dial.txt'\n",
    "FILE_DM_MONO_ATT = 'data_corpus/french_DM_att_mono.txt'\n",
    "\n",
    "run_gen('dm_dial_sem_replaced', LAN, FILE_DM_DIAL_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "run_gen('dm_dial_sem_shuffled', LAN, FILE_DM_DIAL_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "\n",
    "run_gen('dm_mono_sem_replaced', LAN, FILE_DM_MONO_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "run_gen('dm_mono_sem_shuffled', LAN, FILE_DM_MONO_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "\n",
    "run_gen('dm_dial_att_replaced', LAN, FILE_DM_DIAL_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "run_gen('dm_dial_att_shuffled', LAN, FILE_DM_DIAL_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "\n",
    "run_gen('dm_mono_att_replaced', LAN, FILE_DM_MONO_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "run_gen('dm_mono_att_shuffled', LAN, FILE_DM_MONO_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "05e87498-2ec8-4680-b725-82ff5dc7bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LAN = 'en'\n",
    "FP_ITEMS= ['uh', 'um']\n",
    "SP_ITEMS = ['#']\n",
    "FILE_FP = 'data_corpus/english_fp_base_500_checked.txt'\n",
    "\n",
    "run_gen('filler_moved', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('filler_shuffled', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('filler_swapped', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "\n",
    "FILE_REP = 'data_corpus/english_repeats_base_500_checked.txt'\n",
    "\n",
    "run_gen('rep_moved', LAN, FILE_REP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('rep_shuffled', LAN, FILE_REP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "\n",
    "FILE_SP = 'data_corpus/english_sp_base_500_checked.txt'\n",
    "\n",
    "run_gen('pause_moved', LAN, FILE_SP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('pause_shuffled', LAN, FILE_SP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "00c24182-1660-4472-bf59-6909760ee155",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DM_DIAL_SEM = 'data_corpus/english_DM_SEM_dial.txt'\n",
    "FILE_DM_MONO_SEM = 'data_corpus/english_DM_SEM_mono.txt'\n",
    "FILE_DM_DIAL_ATT = 'data_corpus/english_DM_ATT_dial.txt'\n",
    "FILE_DM_MONO_ATT = 'data_corpus/english_DM_ATT_mono.txt'\n",
    "\n",
    "DM_ATT = ['oh','like','well']\n",
    "DM_SEM = ['so','but',\"because\", 'then']#then\n",
    "\n",
    "run_gen('dm_dial_sem_replaced', LAN, FILE_DM_DIAL_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "run_gen('dm_dial_sem_shuffled', LAN, FILE_DM_DIAL_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "\n",
    "run_gen('dm_mono_sem_replaced', LAN, FILE_DM_MONO_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "run_gen('dm_mono_sem_shuffled', LAN, FILE_DM_MONO_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "\n",
    "run_gen('dm_dial_att_replaced', LAN, FILE_DM_DIAL_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "run_gen('dm_dial_att_shuffled', LAN, FILE_DM_DIAL_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "\n",
    "run_gen('dm_mono_att_replaced', LAN, FILE_DM_MONO_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "run_gen('dm_mono_att_shuffled', LAN, FILE_DM_MONO_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c6960689-1e51-4786-9a3d-14a61b39b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAN = 'zh'\n",
    "FP_ITEMS= ['uhm', 'uhn', 'en', 'NEGE', 'NEIGE', 'nhn', 'NAGE']\n",
    "SP_ITEMS= ['#']\n",
    "\n",
    "comma = '，'\n",
    "\n",
    "FILE_FP = 'data_corpus/mandarin_fp_base_updated_checked.txt'\n",
    "\n",
    "run_gen('filler_moved', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('filler_shuffled', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('filler_swapped', LAN, FILE_FP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "\n",
    "FILE_REP = 'data_corpus/mandarin_repeats_base_updated_checked.txt'\n",
    "\n",
    "run_gen('rep_moved', LAN, FILE_REP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('rep_shuffled', LAN, FILE_REP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "\n",
    "FILE_SP = 'data_corpus/mandarin_sp_base_updated_checked.txt'\n",
    "\n",
    "run_gen('pause_moved', LAN, FILE_SP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n",
    "run_gen('pause_shuffled', LAN, FILE_SP, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "619cbd68-6fe2-4d41-a8ed-475833b356d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DM_DIAL_SEM = 'data_corpus/mandarin_DM_SEM_dial.txt'\n",
    "FILE_DM_MONO_SEM = 'data_corpus/mandarin_DM_SEM_mono.txt'\n",
    "FILE_DM_DIAL_ATT = 'data_corpus/mandarin_DM_ATT_dial.txt'\n",
    "FILE_DM_MONO_ATT = 'data_corpus/mandarin_DM_ATT_mono.txt'\n",
    "\n",
    "DM_SEM = ['所以','但是','因為','然後', '可是', '不過', '而且'] #'and'\n",
    "DM_ATT = ['喔', '哦', '就是', '像', '就', '就是說']\n",
    "\n",
    "DM_ATT = {'喔': ['就是', '像', '就', '就是說'],\n",
    "          '哦': ['就是', '像', '就', '就是說'],\n",
    "          '就是': ['喔', '哦', '像',],\n",
    "          '像': ['喔', '哦', '就是', '就', '就是說'],\n",
    "          '就': ['喔', '哦', '像'],\n",
    "          '就是說': ['喔', '哦', '像'],}\n",
    "\n",
    "\n",
    "DM_SEM = {'所以':  ['但是','因為','然後', '可是', '不過', '而且'],\n",
    "          '但是':  ['所以','因為','然後',  '而且'],\n",
    "          '因為':  ['所以','但是','然後', '可是', '不過', '而且'],\n",
    "          '然後':  ['所以','但是','因為', '可是', '不過', '而且'],\n",
    "          '可是':  ['所以','因為','然後', '而且'],\n",
    "          '不過':  ['因為','然後', '所以', '而且'],\n",
    "          '而且':  ['所以','但是','因為','然後', '可是', '不過']    \n",
    "}\n",
    "\n",
    "\n",
    "run_gen('dm_dial_sem_replaced', LAN, FILE_DM_DIAL_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "run_gen('dm_dial_sem_shuffled', LAN, FILE_DM_DIAL_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "\n",
    "run_gen('dm_mono_sem_replaced', LAN, FILE_DM_MONO_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "run_gen('dm_mono_sem_shuffled', LAN, FILE_DM_MONO_SEM, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_SEM)\n",
    "\n",
    "run_gen('dm_dial_att_replaced', LAN, FILE_DM_DIAL_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "run_gen('dm_dial_att_shuffled', LAN, FILE_DM_DIAL_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "\n",
    "run_gen('dm_mono_att_replaced', LAN, FILE_DM_MONO_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)\n",
    "run_gen('dm_mono_att_shuffled', LAN, FILE_DM_MONO_ATT, FP_ITEMS, SP_ITEMS, './data_benchmark/disfl_comma/', comma, 10, DM_ATT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ff977040-10f0-427c-b945-13101ba5ae3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3f5ec-5a1d-4f65-824b-edba8c33b10f",
   "metadata": {},
   "source": [
    "# DISCOURSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fe03e63e-e0cc-4f05-ae15-2a57d184d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3da9d-ae9c-49ff-8efd-821d3a1f0942",
   "metadata": {},
   "source": [
    "## French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997210de-df31-4b52-8eb8-332e491e969e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858446d-a9fc-4400-9914-289a09f3e909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a2eb6a1-ffa1-4901-98b2-ff5ad8634c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FR\n",
    "LAN = 'fr'\n",
    "FILE_DM_DIAL_SEM = 'data_corpus/french_DM_sem_dial.txt'\n",
    "FILE_DM_DIAL_ATT = 'data_corpus/french_DM_att_dial.txt'\n",
    "\n",
    "DM_SEM = ['donc','mais','alors','après'] \n",
    "DM_ATT =  [\"ah\", \"ben\", \"oh\", \"enfin\", \"bon\"]#[\"ah\", \"ben\"]\n",
    "\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_DIAL_SEM,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(N_EXPS):\n",
    "                                                 \n",
    "        DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','dm','right'])\n",
    "        #print(DM_DF['dm'])\n",
    "\n",
    "        DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "        DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)    \n",
    "        \n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['replaced_dm'])            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_sem_dial\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_dial_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "                        \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_sem_dial\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_dial_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_DIAL_ATT,'r', encoding = 'utf-8').readlines()]\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','dm','right'])\n",
    "\n",
    "        \n",
    "        DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "        DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)    \n",
    "        \n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_ATT,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        DM_DF['mistyped_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "\n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['replaced_dm'])            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_att_dial\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_dial_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_att_dial\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_dial_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850e5a6-8dd8-4767-be10-7a52204e917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "19aac7b0-bcd3-4514-8416-852c06befd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DM_MONO_SEM = 'data_corpus/french_DM_sem_mono.txt'\n",
    "FILE_DM_MONO_ATT = 'data_corpus/french_DM_att_mono.txt'\n",
    "\n",
    "DM_SEM = ['donc','mais','alors','après'] #['donc','mais'] \n",
    "DM_ATT = [\"ah\", \"ben\", \"oh\", \"enfin\", \"bon\"]\n",
    "\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_MONO_SEM,'r', encoding = 'utf-8').readlines()]\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(N_EXPS):\n",
    "                                                 \n",
    "        DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','dm','right'])\n",
    "\n",
    "        #print(DM_DF['dm'])\n",
    "\n",
    "        DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "        DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)    \n",
    "        \n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['replaced_dm'])            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_sem_mono\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_mono_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_sem_mono\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_mono_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_MONO_ATT,'r', encoding = 'utf-8').readlines()]\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','dm','right'])\n",
    "\n",
    "        \n",
    "        DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "        DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)    \n",
    "        \n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_ATT,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        DM_DF['mistyped_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['replaced_dm'])            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_att_mono\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_mono_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_att_mono\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_mono_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1c7bb-db17-4648-b61c-65e2169bebff",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0d441-fdcb-4901-b288-eac4a3c069a8",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f9dd5d76-06f7-4e0b-bd26-d7f3aa42830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EN\n",
    "LAN = 'en'\n",
    "FILE_DM_DIAL_SEM = 'data_corpus/english_DM_SEM_dial.txt'\n",
    "FILE_DM_MONO_SEM = 'data_corpus/english_DM_SEM_mono.txt'\n",
    "FILE_DM_DIAL_ATT = 'data_corpus/english_DM_ATT_dial.txt'\n",
    "FILE_DM_MONO_ATT = 'data_corpus/english_DM_ATT_mono.txt'\n",
    "DM_ATT = ['oh','like','well']\n",
    "DM_SEM = ['so','but',\"because\", 'then']#then\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_DIAL_SEM,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['text'] = DM_DF['text'].apply(lowercase_except_I_and_contractions)\n",
    "\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "\n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\" , alt[1], row['replaced_dm'])\n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_sem_dial\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_dial_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_sem_dial\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_dial_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_DIAL_ATT,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['text'] = DM_DF['text'].apply(lowercase_except_I_and_contractions)\n",
    "\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_ATT,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\" , alt[1], row['replaced_dm'])\n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_att_dial\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_dial_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_att_dial\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_dial_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "871a16b2-76eb-4d0e-8802-648fc17c482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_MONO_SEM,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['text'] = DM_DF['text'].apply(lowercase_except_I_and_contractions)\n",
    "\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\" , alt[1], row['replaced_dm'])\n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_sem_mono\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_mono_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_sem_mono\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_mono_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "for alt in alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_MONO_ATT,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['text'] = DM_DF['text'].apply(lowercase_except_I_and_contractions)\n",
    "\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm,dmlist=DM_ATT,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\" , alt[1], row['replaced_dm'])\n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_att_mono\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_mono_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\"( *)#\", alt[1], row['text'])\n",
    "            bad = re.sub(\"( *)#\", alt[1], row['shuffled'])\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_att_mono\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_mono_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e493c-1c61-410c-b12e-89a9542ab018",
   "metadata": {},
   "source": [
    "## Mandarin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "401ebb22-28ca-449e-bd3a-352d14c387fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ZH\n",
    "LAN = 'zh'\n",
    "FILE_DM_DIAL_SEM = 'data_corpus/mandarin_DM_SEM_dial.txt'\n",
    "FILE_DM_MONO_SEM = 'data_corpus/mandarin_DM_SEM_mono.txt'\n",
    "FILE_DM_DIAL_ATT = 'data_corpus/mandarin_DM_ATT_dial.txt'\n",
    "FILE_DM_MONO_ATT = 'data_corpus/mandarin_DM_ATT_mono.txt'\n",
    "DM_SEM = ['所以','但是','因為','然後', '可是', '不過', '而且'] #'and'\n",
    "DM_ATT = ['喔', '哦', '就是', '像', '就', '就是說']\n",
    "\n",
    "DM_ATT = {'喔': ['就是', '像', '就', '就是說'],\n",
    "          '哦': ['就是', '像', '就', '就是說'],\n",
    "          '就是': ['喔', '哦', '像',],\n",
    "          '像': ['喔', '哦', '就是', '就', '就是說'],\n",
    "          '就': ['喔', '哦', '像'],\n",
    "          '就是說': ['喔', '哦', '像'],}\n",
    "\n",
    "\n",
    "DM_SEM = {'所以':  ['但是','因為','然後', '可是', '不過', '而且'],\n",
    "          '但是':  ['所以','因為','然後',  '而且'],\n",
    "          '因為':  ['所以','但是','然後', '可是', '不過', '而且'],\n",
    "          '然後':  ['所以','但是','因為', '可是', '不過', '而且'],\n",
    "          '可是':  ['所以','因為','然後', '而且'],\n",
    "          '不過':  ['因為','然後', '所以', '而且'],\n",
    "          '而且':  ['所以','但是','因為','然後', '可是', '不過']    \n",
    "}\n",
    "\n",
    "for alt in zh_alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split(' <$> ') for u in open(FILE_DM_DIAL_SEM,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    #DM_DF['left'] = DM_DF['left'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    #DM_DF['right'] = DM_DF['right'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    \n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm_dict,dmlist=DM_SEM,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        DM_DF['text'] = DM_DF.apply(clean_zh_space,col='text',axis=1)\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(clean_zh_space,col='replaced_dm',axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(clean_zh_space,col='shuffled',axis=1)\n",
    "\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['replaced_dm'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_sem_dial\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_dial_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['shuffled'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_sem_dial\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_dial_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "\n",
    "for alt in zh_alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split('<$>') for u in open(FILE_DM_DIAL_ATT,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    #DM_DF['left'] = DM_DF['left'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    #DM_DF['right'] = DM_DF['right'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    \n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm_dict,dmlist=DM_ATT,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "        DM_DF['mistyped_dm'] = DM_DF.apply(replace_dm,dmlist=DM_SEM,axis=1)\n",
    "\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        DM_DF['text'] = DM_DF.apply(clean_zh_space,col='text',axis=1)\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(clean_zh_space,col='replaced_dm',axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(clean_zh_space,col='shuffled',axis=1)\n",
    "        DM_DF['mistyped_dm'] = DM_DF.apply(clean_zh_space,col='mistyped_dm',axis=1)\n",
    "        \n",
    "\n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['replaced_dm'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_att_dial\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_dial_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['shuffled'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_att_dial\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_dial_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f4a49027-dff4-4b6d-82d1-9350272f310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alt in zh_alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split(' <$> ') for u in open(FILE_DM_MONO_SEM,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    #DM_DF['left'] = DM_DF['left'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    #DM_DF['right'] = DM_DF['right'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    \n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm_dict,dmlist=DM_SEM,axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        DM_DF['text'] = DM_DF.apply(clean_zh_space,col='text',axis=1)\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(clean_zh_space,col='replaced_dm',axis=1)\n",
    "        DM_DF['shuffled'] = DM_DF.apply(clean_zh_space,col='shuffled',axis=1)\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['replaced_dm'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_sem_mono\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_mono_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['shuffled'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_sem_mono\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_mono_sem_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "\n",
    "for alt in zh_alts:\n",
    "    folder = alt[0]    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    DM_EXAMPLES = [u.split(' <$> ') for u in open(FILE_DM_MONO_ATT,'r', encoding = 'utf-8').readlines()]\n",
    "        \n",
    "    DM_DF = pd.DataFrame(DM_EXAMPLES,columns=['left','right'])\n",
    "    DM_DF['dm'] = DM_DF.apply(get_dm,axis=1)\n",
    "    DM_DF['left'] = DM_DF['left'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    DM_DF['right'] = DM_DF['right'].str.replace(\"#( #)+\", \"# \", regex = True)\n",
    "    #DM_DF['left'] = DM_DF['left'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    #DM_DF['right'] = DM_DF['right'].str.replace(\"SOUTHERN_MIN\", \"*\", regex = True)\n",
    "    \n",
    "    \n",
    "    for i in range(N_EXPS):\n",
    "        DM_DF['text'] = DM_DF['left'].str.strip() + ' # '+  DM_DF['right'].str.strip()\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(replace_dm_dict,dmlist=DM_ATT,axis=1)\n",
    "\n",
    "        DM_DF['shuffled'] = DM_DF.apply(shuffle_utterance,axis=1)\n",
    "\n",
    "        \n",
    "        exp_tag = LAN+'_'+str(i)\n",
    "\n",
    "        DM_DF['text'] = DM_DF.apply(clean_zh_space,col='text',axis=1)\n",
    "        DM_DF['replaced_dm'] = DM_DF.apply(clean_zh_space,col='replaced_dm',axis=1)\n",
    "\n",
    "        DM_DF['shuffled'] = DM_DF.apply(clean_zh_space,col='shuffled',axis=1)\n",
    "        \n",
    "        #REPLACED\n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['replaced_dm'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "            \n",
    "            item_replaced = {\"sentence_good\":good,\"sentence_bad\":bad, \"field\": \"discourse_att_mono\", \n",
    "                             \"linguistics_term\": \"replaced_\"+LAN, \"UID\": \"replaced_\"+exp_tag, \"simple_LM_method\": True, \n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \"lexically_identical\": False, \n",
    "                             \"pair_id\": str(cnt)}\n",
    "            result.append(item_replaced)\n",
    "            cnt +=1\n",
    "                \n",
    "        with open(folder + 'replaced_dm_mono_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        \n",
    "        # SHUFFLED    \n",
    "        result = []\n",
    "        cnt=0\n",
    "        for i,row in DM_DF.iterrows():\n",
    "            good = re.sub(\" *#\", alt[1], row['text'])\n",
    "            bad = re.sub(\" *#\", alt[1], row['shuffled'])\n",
    "            #good = re.sub(\" *\\\\*\", \"\", good)\n",
    "            #bad = re.sub(\" *\\\\*\", \"\", bad)\n",
    "\n",
    "            \n",
    "            item_moved = {\"sentence_good\":good, \"sentence_bad\":bad, \"field\": \"discourse_att_mono\", \n",
    "                             \"linguistics_term\": \"shuffled_\"+LAN, \"UID\": \"shuffled_\"+exp_tag, \"simple_LM_method\": True,\n",
    "                             \"one_prefix_method\": False, \"two_prefix_method\": False, \n",
    "                             \"lexically_identical\": True, \"pair_id\": str(cnt)}\n",
    "            result.append(item_moved)\n",
    "            cnt +=1\n",
    "                        \n",
    "        with open(folder + 'shuffled_dm_mono_att_' + exp_tag + '.json', 'w', encoding='utf-8') as outfile:\n",
    "            for entry in result:\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "96663b30-54e5-4ecf-984c-3875ddb22730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8e103993-6ca2-4d5a-87e3-550e922b2270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'導致他們現在都不敢出來說一講什麼，就吵啊把他那個解讀啊解成不一樣害他們都嚇到不是嗎'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = \"導致他們現在都不敢出來說*一講什麼，就吵啊把他那個解讀啊解成不一樣害他們都嚇到不是嗎\"\n",
    "bad = re.sub('(@|\\\\*) *', '', bad)\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fe024-9abf-48b0-a597-5f7a93ed7a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c054e-2920-47ef-8f5a-5c65efd44e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57ce87-b1e2-4d1c-8a91-eb28d5128599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edc283-be60-4630-b811-61a3f29eaf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809a91f-32e8-4d1c-970d-13f5354e6a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb10495-3c3e-4f17-8b4f-8e27a5dc7eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
